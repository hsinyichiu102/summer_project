# -*- coding: utf-8 -*-
"""model_preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gOgfpIZq23pqUkGmr9JBHM7gCWkIxjDy
"""

!nvidia-smi
!kill process_id
!kill -9 PID1 PID2

import gc
gc.collect()

from google.colab import drive
drive.mount('/content/gdrive')

"""# Data mugging


1. downloaded the two file(AMAZON_FASHION.json and meta_Amazon_Fashion.json) from the link
2. merge the two file on the product id(asin)
3. create the train and test file named as x1,x2,y1,y2
4. We would like to know that if the result would be different if we used the different amount of the data to tran, therefore there are 265,436 items in x1; 309,657 items in x2; 176,958 in y1; 132,719 in y2
"""

import json
import gzip
!wget 'http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/AMAZON_FASHION.json.gz'
data1 = []
with gzip.open('AMAZON_FASHION.json.gz') as f:
    for l in f:
        data1.append(json.loads(l.strip()))

reviewer = pd.DataFrame.from_dict(data1)

!wget 'http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles/meta_AMAZON_FASHION.json.gz'

data = []
with gzip.open('meta_AMAZON_FASHION.json.gz') as f:
    for l in f:
        data.append(json.loads(l.strip()))

product = pd.DataFrame.from_dict(data)

product_merge= pd.merge(product,reviewer,on='asin',how='inner')
print(len(product_merge))

from sklearn.utils import shuffle
product_merge=product_merge[['asin','brand','title','description','feature','reviewerID','reviewText']]
print(len(product_merge))
x,y= train_test_split(product_merge,test_size=0.5,shuffle=True)
print(len(x))
print(len(y))
x1,y1 = train_test_split(x,test_size=0.4,shuffle=True)
print(len(x1)) #265436
x1.to_csv('/content/gdrive/My Drive/file/mung_data/x1.csv',index=False,header=True)
y1.to_csv('/content/gdrive/My Drive/file/mung_data/y1.csv',index=False,header=True)
print(len(y1)) #176958
x2,y2 = train_test_split(y,test_size=0.3,shuffle=True)
x2.to_csv('/content/gdrive/My Drive/file/mung_data/x2.csv',index=False,header=True)
y2.to_csv('/content/gdrive/My Drive/file/mung_data/y2.csv',index=False,header=True)
print(len(x2)) #309675
print(len(y2)) #132719

"""# Data preprocessing

1.   we took all the product from x1 and x2 file to put all the information of the product to a row
2.   That is there is no duplicate product id in our train dataset
3. by uing the preprocess_for_query method to get the base form of each word in the content
"""

import copy
import pickle

import pandas as pd
import numpy as np
import sys
import random
from gensim.parsing import preprocess_string

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

from nltk import word_tokenize, WordNetLemmatizer, SnowballStemmer
from nltk.corpus import stopwords
from sklearn.metrics.pairwise import linear_kernel
from sklearn.model_selection import train_test_split

import re
def preprocess_for_query(sentence):
    """
    clear the query sentence
    :param sentence: the query
    :return: a sentence only with meaningful words
    """
    sentence= re.sub(r'([^\s\w]|\\_)+','',sentence)
    lemm= WordNetLemmatizer()

    stemmer = SnowballStemmer('english')
    word_tokens = word_tokenize(sentence)
    filtered_sentence = [w for w in word_tokens if not w in stopwords.words('english')]
    filtered_sentence= [stemmer.stem(w) for w in filtered_sentence]
    words=' '.join(lemm.lemmatize(w) for w in filtered_sentence)
    return words

def flaten_data(file):
    """
    a helper function to collect all the data in the separated columns into one columns called bag_of_words
    save the data in to word_bag.csv
    :param file: the dataset
    :return: save the useful data
    """
    n=0
    print(file.shape)
    dict={}
    for index, row in file.iterrows():
        title = row['title']
        brand = row['brand']
        description= row['description']
        description= ''.join(description)
        
        feature = row['feature']
        feature = ''.join(feature)
        
        reviewText= row['reviewText']
        
        asin= row['asin']

        if asin in dict.keys():
            words= dict[asin]
            new_words= words+' '+ reviewText
            dict[asin]=new_words

        else:
            words= title+' '+brand+' '+description+' '+feature+' '+reviewText
            dict[asin]=words
        n=n+1
        
    print(n)
    pf= pd.DataFrame(columns={'asin','Bag_of_words'})
    for key,value in dict.items():
      pf= pf.append({'asin':key, 'Bag_of_words':value},ignore_index=True)
    
    for index, row in pf.iterrows():
      text= preprocess_for_query(row['Bag_of_words']) #using the preprocess_for_query to receive the base form of each word
      train.loc[index, 'Bag_of_words_capture'] = text
      
    print("start to store")
    pf.to_csv('/content/gdrive/My Drive/file/clear_data/x2_word_bag.csv',index=False,header=True)
    print("file done")
    
    return pf

"""# Data preprocess for test dataset

1.   in order to receive more preceise score from our program, we should filter the item in the test data and not in the train data
2.   we use the all information about a product to test our model, therefore we collected the title, brand, description, feature and reviewText from the row to put into a new column
"""

"""
In order to receive more accurate score, 
this program is to filter the product in the test data that not in the train data
"""
x =pd.read_csv('/content/gdrive/My Drive/file/clear_data/x2_word_bag.csv_10000')
print(len(x)) #50332
y= pd.read_csv('/content/gdrive/My Drive/file/mung_data/y2.csv')
print(len(y)) #110598
asin=x['asin']

merge_x_y= pd.merge(y,asin, on='asin',how='inner')
print(len(merge_x_y))

merge_x_y.to_csv('/content/gdrive/My Drive/file/mung_data/y2.csv',index=False,header=True)
print('done')

"""
we put the all content of a product into a column, which can present the requirement from user
"""
test= pd.read_csv('/content/gdrive/My Drive/file/mung_data/y2.csv')
print(len(test))
test=test.fillna('no')
test['sentence']=''
n=len(test)
for index, row in test.iterrows():
  
  asin= row['asin']

  title = row['title']
    

  brand = row['brand']


  description= row['description']
  description= ''.join(description)
    
  feature = row['feature']
  feature = ''.join(feature)
    
  reviewText= row['reviewText']
    

  text= title+brand+description+feature+reviewText
  text= preprocess_for_query(text)
  test.loc[index,'sentence']=text
  temp=n-index
  print(temp)

test=test[['asin','sentence']]
print(test.head(2))

test.to_csv('/content/gdrive/My Drive/file/mung_data/y22.cs',index=False,header=True)
print('done')

"""# TFIDF MODEL TRAINING



1.   By using the method from sklearn to create the tfidf model
2.   the code is refer from kaggle "Searching the million news with TF-IDF" :
https://www.kaggle.com/chechir/searching-the-million-news-with-tf-idf
"""

import pickle
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer

def tfidf(df):
    """
    We used the TF-IDF package from SKlearn to create our model. 
    CountVectorizer method is to get the term frequency of a word in the content; 
    TfidfTransformer method is to have the idf score and the final weight. 
    By using the pickle method to store our model and use it in the search engine later.
    :param df: the dataset
    :return: three model of the vectroize, tfidf-fit and tfidf-transform
    """
    print('start')
    df=df['Bag_of_words_capture']

    vectoriser = CountVectorizer(stop_words = stopwords.words('english'), ngram_range=(1,3))

    train_vectorised= vectoriser.fit_transform(df)
    pickle.dump(vectoriser, open('/content/gdrive/My Drive/file/tfidf_model/x11_vect', 'wb'))

    transformer = TfidfTransformer()
    transformer.fit(train_vectorised)
    pickle.dump(transformer,open('/content/gdrive/My Drive/file/tfidf_model/x11_tfidf','wb'))

    fitted_tfidf = transformer.transform(train_vectorised)
    pickle.dump(fitted_tfidf, open('/content/gdrive/My Drive/file/tfidf_model/x11_fitted_tfidf', 'wb'))
    print('done')

df=pd.read_csv('/content/gdrive/My Drive/file/clear_data/x11_word_bag.csv')
tfidf(df)

"""**Using the Counter to get the TF score of each word**"""

df=df['Bag_of_words_capture']
print(len(df))
vectoriser = CountVectorizer(stop_words = stopwords.words('english'), ngram_range=(1,3))
train_vectorised= vectoriser.fit_transform(df)

from collections import Counter
vocab= vectoriser.get_feature_names()
counts= train_vectorised.sum(axis=0).A1

freq_dis= Counter(dict(zip(vocab,counts)))
for i in freq_dis.most_common(10):
  print (i)

"""**Display the words in our wordbag by using the worldcloud**"""

vectoriser= pickle.load(open('/content/gdrive/My Drive/file/tfidf_model/x11_vect_train','rb'))

text= vectoriser.get_feature_names()
print(type(text))
text = ''.join(str(e) for e in text)
print(type(text))

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from wordcloud import WordCloud
wordcloud = WordCloud(width=1600, height=800, max_font_size=200).generate(text)
plt.figure(figsize=(12,10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""# Word2Vec and Doc2Vec model training
1. Using the Doc2Vec to vectorize each product in our dataset
2. Using the Word2Vec to vectorize each word in the dataset
"""

from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from gensim.models import Word2Vec
from gensim.parsing.preprocessing import preprocess_string

def train_d2v_w2v_model(X):
    """
    train the d2v and w2v model
    :param X: the dataset
    :return: d2v and w2v model
    """
    pf= X
    pf['product_tag']=[TaggedDocument(words=pf.Bag_of_words_capture[i].split(),tags=[i])for i in range(pf.shape[0])]
    print(pf['product_tag'].head())
    #train doc2vec model
    print("start")
    model_doc2vec = Doc2Vec(pf.product_tag.values, dm=1, vector_size=200, window=5, min_count=1, workers=8, epochs=5)
    model_doc2vec.save("/content/gdrive/My Drive/file/d2v_x11.model")
    print('done')
    #train word2vec model
    pf['Bag_of_words_capture']=pf['Bag_of_words_capture'].apply(preprocess_string)
    model_word2vec = Word2Vec(sentences=pf.Bag_of_words_capture, size=100,window=5,min_count=1, seed=1,  sg =0)
    model_word2vec.save("/content/gdrive/My Drive/file/w2v_x11.model")
    print("done")

product= pd.read_csv('/content/gdrive/My Drive/file/clear_data/x11_word_bag.csv')
product.head()
train_d2v_w2v_model(product)

"""**checking with some word to ensure if the similar word is sensible**


> we need to use the concept female+man = male+woman to get the the correct gender
"""

d2v_model= Doc2Vec.load('/content/gdrive/My Drive/file/d2v_x11.model')

d2v_model.wv.most_similar_cosmul(positive = ["dress"])

w2v_model= Word2Vec.load('/content/gdrive/My Drive/file/w2v_x11.model')
w2v_model.most_similar(positive=['woman','male'], negative=['man'])